{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a9bb3cb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import jieba\n",
    "import re\n",
    "import random\n",
    "import numpy as np\n",
    "from sklearn.cluster import KMeans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "d7e190ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Text_preprocessing(txt,word_flag):  #文本的预处理工作\n",
    "    ad = ['本书来自www.cr173.com免费txt小说下载站\\n更多更新免费电子书请关注www.cr173.com', '----〖新语丝电子文库(www.xys.org)〗', '新语丝电子文库']\n",
    "    for i in ad:  #删去广告\n",
    "        txt = txt.replace(i, '')\n",
    "\n",
    "    with open(\"cn_stopwords.txt\", \"r\", encoding=\"utf-8\") as file:        #删去停词，但也会带走一些实词\n",
    "        stop = file.read()\n",
    "    stop = stop.split()\n",
    "\n",
    "    word_flag = word_flag #按词或者字切分                                     #1是字\n",
    "    if word_flag == 1:\n",
    "        txt = jieba.lcut(txt)\n",
    "    else:\n",
    "        txt = [one for one in txt]\n",
    "    txt = [val for val in txt if val not in stop]  # 去除停词\n",
    "\n",
    "    txt = [i for i in txt if not re.findall(\"[^\\u4e00-\\u9fa5]+\", i)]\n",
    "\n",
    "    return txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "id": "90462168",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read(menu,wordflag):\n",
    "    data = []\n",
    "\n",
    "    for name in menu:\n",
    "        data_1 = []\n",
    "        path = 'Data/' + name + '.txt'\n",
    "        with open(path, 'r', encoding='ANSI') as read_file:\n",
    "            read_cu = read_file.read()\n",
    "            read_cu = Text_preprocessing(read_cu,wordflag)\n",
    "            read_list = list(read_cu)\n",
    "            cut = int(len(read_cu) // 13)\n",
    "            for i in range(13):\n",
    "                data_0 = read_list[i * cut + 1:i * cut + 500]\n",
    "                data_1 = data_1 + read_list[i * cut + 501:i * cut + 1000]\n",
    "                data.append(data_0)\n",
    "            #print(read_list)\n",
    "            #print(name)\n",
    "            read_file.close()\n",
    "\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "id": "3286a9b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def result_sort(result_topic_p_ti,num,topic_num):  #某次循环后的结果    第num个 段  共有多少个topic \n",
    "    paixu=topic_p_t[num]\n",
    "    paixuhou=sorted(paixu,reverse=True)\n",
    "    xnum1=np.where(paixu==paixuhou[0])\n",
    "    xnum2=np.where(paixu==paixuhou[1])\n",
    "    xnum3=np.where(paixu==paixuhou[2])#前三个最大的主题\n",
    "    print('topic1',xnum1[0],'confidence',paixu[xnum1],end=' ')\n",
    "    print('topic2',xnum2[0],'confidence',paixu[xnum2],end=' ')\n",
    "    #print('topic3',xnum3[0],'confidence',paixu[xnum3])\n",
    "    return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "id": "67b592be",
   "metadata": {},
   "outputs": [],
   "source": [
    "wordflag=2#切换字词切分的标志  1是词\n",
    "\n",
    "topic_set = []  # 所有主题合集\n",
    "topic_cnt = {}  # 每个主题中的词数\n",
    "alltopic = []  # 所有文章词的主题序列\n",
    "alltopic_dis = []  # 所有文章各个主题的数量\n",
    "word_cnt = []  # 所有文章的总词数\n",
    "with open('Data/inf.txt', \"r\", encoding=\"ANSI\") as file:  # 读入目录\n",
    "    menu = file.read()    #menu是小说名字的目录\n",
    "    menu = menu.split(',')\n",
    "    data = read(menu,wordflag)    #data 是小说内的词语"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "id": "b41493ed",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "wordflag=2#切换字词切分的标志  1是词\n",
    "topic_set = []  # 所有主题合集\n",
    "topic_cnt = {}  # 每个主题中的词数\n",
    "alltopic = []  # 所有文章词的主题序列\n",
    "alltopic_dis = []  # 所有文章各个主题的数量\n",
    "word_cnt = []  # 所有文章的总词数\n",
    "topic_num =10  # 主题 数量\n",
    "for i in range(topic_num):\n",
    "    topics = {}\n",
    "    topic_set.append(topics)\n",
    "#(topic_set)主题序列\n",
    "#(topic_set[1])第一个主题序列，目前全是空主题\n",
    "for txt in data:\n",
    "    topic = []  # 文章词的topic序列\n",
    "    topic_dis = {}  # 每篇文章各个topic的数量\n",
    "    for word in txt:\n",
    "        a = random.randint(0, topic_num - 1)  # 初始随机生成一个topic\n",
    "        topic.append(a)\n",
    "        topic_cnt[a] = topic_cnt.get(a, 0) + 1        #每个词语都有一个随机的topic\n",
    "        topic_dis[a] = topic_dis.get(a, 0) + 1\n",
    "        topic_set[a][word] = topic_set[a].get(word, 0) + 1\n",
    "    alltopic.append(topic)\n",
    "    topic_dis = list(\n",
    "        dict(sorted(topic_dis.items(), key=lambda x: x[0], reverse=False)).values())  # 每篇文章各个topic的数量,排序并转化为list\n",
    "    alltopic_dis.append(topic_dis)\n",
    "    word_cnt.append(sum(topic_dis))\n",
    "    sorted_toic_cnt=topic_cnt\n",
    "# print(sorted_toic_cnt)                     #排序后的主题数量   此时为随机分\n",
    "topic_cnt = list(dict(sorted(topic_cnt.items(), key=lambda x: x[0], reverse=False)).values())\n",
    "# print('topic_cnt')\n",
    "# print(topic_cnt)                   #按顺序的主题数量"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "id": "e9b9f6ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "alltopic_dis = np.array(alltopic_dis)\n",
    "topic_cnt = np.array(topic_cnt)\n",
    "word_cnt = np.array(word_cnt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "id": "3eac400b",
   "metadata": {},
   "outputs": [],
   "source": [
    "All_topics = {}\n",
    "for i in range(topic_num):\n",
    "    All_topics.update(topic_set[i])\n",
    "K = len(All_topics)\n",
    "# print('K', K)   字典总长度"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "id": "030e027e",
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_p = []  # 文章选中topic的概率\n",
    "topic_p_t = []\n",
    "for i in range(len(data)):\n",
    "    p = np.divide(alltopic_dis[i], word_cnt[i])\n",
    "    topic_p.append(p)\n",
    "topic_p = np.array(topic_p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "id": "c128c9e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loop_num: 5\n",
      "loop_num: 10\n",
      "loop_num: 15\n",
      "loop_num: 20\n"
     ]
    }
   ],
   "source": [
    "result_topic_set={}\n",
    "result_topic_p_t={}\n",
    "for i in range(int(topic_num/5+1)):\n",
    "    result_topic_set[i]={}\n",
    "    result_topic_p_t[i]={}\n",
    "stop = 0  # 迭代停止标志\n",
    "loopcount = 0  # 迭代次数\n",
    "while loopcount <= 20:  # 使用迭代次数控制循环\n",
    "    # while stop == 0:\n",
    "    i = 0\n",
    "    for txt in data:\n",
    "        topic = alltopic[i]  # 当前文章词topic序列\n",
    "        for w in range(len(txt)):\n",
    "            word = txt[w]\n",
    "            p = []  # 该词由各个topic生成的概率\n",
    "            n_topic2word = []  # 各个topic生成该词的频率\n",
    "            p_topic2word = []  # 各个topic生成该词的概率\n",
    "            for k in range(topic_num):\n",
    "                n_topic2word.append(topic_set[k].get(word, 0))\n",
    "            n_topic2word = np.array(n_topic2word)\n",
    "            alpha = 0.01\n",
    "            beta = 0.1\n",
    "            p_topic2word = (n_topic2word + beta) / (topic_cnt + K * beta)\n",
    "            p = (topic_p[i] + alpha) * p_topic2word\n",
    "\n",
    "            # p_topic2word = n_topic2word / topic_cnt\n",
    "            # p = topic_p[i] * p_topic2word\n",
    "\n",
    "            # max = np.argmax(p)  # 生成该词最大可能的topic\n",
    "            max = np.random.choice(topic_num, p=p / p.sum())\n",
    "            # if max!=topic[w]:\n",
    "            #    print('changed')\n",
    "\n",
    "            ## 更新各文章中各topic数量\n",
    "            alltopic_dis[i][topic[w]] -= 1\n",
    "            alltopic_dis[i][max] += 1\n",
    "            ## 更新每个topic的总词数\n",
    "            topic_cnt[topic[w]] -= 1\n",
    "            topic_cnt[max] += 1\n",
    "            ## 更新各个topic 内容\n",
    "            topic_set[topic[w]][word] = topic_set[topic[w]].get(word, 0) - 1\n",
    "            topic_set[max][word] = topic_set[max].get(word, 0) + 1\n",
    "            ## 新topic序列\n",
    "            topic[w] = max\n",
    "        alltopic[i] = topic\n",
    "        i += 1\n",
    "    loopcount += 1\n",
    "    if loopcount == 1:  # 更新新的文章对topic概率\n",
    "        for i in range(len(data)):\n",
    "            p = np.divide(alltopic_dis[i], word_cnt[i])\n",
    "            topic_p_t.append(p)\n",
    "        topic_p_t = np.array(topic_p_t)\n",
    "    else:\n",
    "        for i in range(len(data)):\n",
    "            p = np.divide(alltopic_dis[i], word_cnt[i])\n",
    "            topic_p_t[i] = p\n",
    "    if (topic_p_t == topic_p).all():\n",
    "        stop = 1\n",
    "    else:\n",
    "        topic_p = topic_p_t.copy()\n",
    "    if loopcount==0:\n",
    "        result_topic_set[0]=topic_set\n",
    "        result_topic_p_t[0]=topic_p_t\n",
    "    if loopcount%5==0:    \n",
    "        print('loop_num:', loopcount)\n",
    "        result_topic_set[int(loopcount/5)]=topic_set\n",
    "        result_topic_p_t[int(loopcount/5)]=topic_p_t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd5e61c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(topic_num):\n",
    "    print('topic:', i+1)\n",
    "    a = topic_set[i]\n",
    "    res = {}\n",
    "    for key, value in a.items():  # 每个topic降序排序\n",
    "        if value != 0:\n",
    "            res[key] = value\n",
    "    res = list(sorted(res.items(), key=lambda x: x[1], reverse=True))\n",
    "    print(res[:3])  # 输出前5个最多的主题词\n",
    "\n",
    "# print(topic_p_t)\n",
    "loop_num=int(loopcount/5)        #第几次循环结果  一共有  5 10  15  20 25 30 35几种\n",
    "paragraph_num=208  #一共208个段落\n",
    "for i in range(0,paragraph_num,40):\n",
    "#     for j in range(1,loop_num+1):\n",
    "        print('loop_num=',j,'paragraph_num=',(loop_num+1)*5,':')\n",
    "        result_sort(result_topic_p_t[loop_num],i, topic_num) #某次循环后的结果    第num个 段落  共有多少个topic \n",
    "        print()\n",
    "\n",
    "cluster = KMeans(n_clusters=16)  # K-means聚类\n",
    "cluster.fit(topic_p_t)\n",
    "\n",
    "labels = cluster.labels_\n",
    "for i in range(len(labels)):\n",
    "    print(labels[i], end=' ')\n",
    "    if (i + 1) % 13 == 0:\n",
    "        print('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "id": "1ec6c623",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
